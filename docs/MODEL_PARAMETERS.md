# Model Parameters Configuration Guide

This guide explains all the model parameters you can configure for each AI provider (OpenAI, Anthropic, OpenRouter, Ollama).

## ğŸ“‹ Quick Reference

| Parameter | Range | Default | Description |
|-----------|-------|---------|-------------|
| **Temperature** | 0.0 - 2.0 | 0.1 | Controls response randomness |
| **Max Response Tokens** | 2000 - 16000 | 8000 | Maximum length of AI responses |
| **Context Window** | 8000 - 200000 | Varies | Maximum context size model can process |

---

## ğŸŒ¡ï¸ Temperature

### What It Does
Controls how random or deterministic the AI's responses are.

### Values
- **0.0** - Completely deterministic (same input = same output)
- **0.1** - Very consistent, focused â­ **RECOMMENDED**
- **0.5** - Moderate creativity
- **1.0** - Balanced between creativity and consistency
- **2.0** - Maximum creativity and randomness

### Recommended Settings

#### For Smart Search & Task Chat (JSON Output)
```
Temperature: 0.1
```
**Why:** Lower temperature ensures reliable JSON parsing and structured output. Higher values may cause:
- âŒ Invalid JSON format
- âŒ Inconsistent property extraction
- âŒ Unpredictable keyword expansion

#### For Experimental/Creative Use
```
Temperature: 0.5 - 1.0
```
**Use with caution:** May impact Smart Search and Task Chat reliability.

### Impact by Mode

| Mode | Impact | Recommended |
|------|--------|-------------|
| **Simple Search** | None (doesn't use AI) | N/A |
| **Smart Search** | High (JSON parsing) | 0.1 |
| **Task Chat** | High (JSON + responses) | 0.1 |

---

## ğŸ“Š Max Response Tokens

### What It Does
Sets the maximum length of AI-generated responses. Affects **BOTH** Smart Search query parsing AND Task Chat responses.

### Values
- **2000** - Minimal (may truncate)
- **8000** - Default â­ **RECOMMENDED**
- **12000** - Extended
- **16000** - Maximum

### Why 8000 is Default
- Supports 60 keywords semantic expansion
- Accommodates comprehensive task analysis
- Allows detailed explanations in Task Chat
- Balances performance and cost

### Parameter Names by Provider

| Provider | Parameter Name | Notes |
|----------|---------------|-------|
| **OpenAI** | `max_tokens` | Standard OpenAI API |
| **Anthropic** | `max_tokens` | Same as OpenAI |
| **OpenRouter** | `max_tokens` | OpenAI-compatible |
| **Ollama** | `num_predict` | Different name! |

### When to Increase

**Increase to 12000-16000 if:**
- âœ… You have many configured languages (4+)
- âœ… You want very detailed Task Chat responses
- âœ… You're getting truncated responses
- âœ… Cost is not a primary concern

### When to Decrease

**Decrease to 4000-6000 if:**
- âœ… You want faster responses
- âœ… You want to reduce API costs
- âœ… You only use 1-2 languages
- âœ… You prefer concise responses

### Cost Impact Example

With gpt-4o-mini ($0.150 per 1M output tokens):
- 2000 tokens: $0.0003 per query
- 8000 tokens: $0.0012 per query
- 16000 tokens: $0.0024 per query

---

## ğŸ—‚ï¸ Context Window

### What It Does
Sets the maximum total context size (input prompt + response) the model can process.

### Provider Differences

#### OpenAI / Anthropic / OpenRouter
- **Purpose:** Informational only
- **Behavior:** Context window is determined by the selected model
- **Examples:**
  - gpt-4o-mini: 128,000 tokens
  - claude-sonnet-4: 200,000 tokens
  - Model determines actual limit

#### Ollama
- **Purpose:** Actively used as `num_ctx` parameter
- **Behavior:** You MUST configure this correctly
- **Default:** 32,000 tokens
- **Important:** This is sent to Ollama API

### Critical Relationship

```
Input Prompt + Max Response Tokens â‰¤ Context Window
```

**Example:**
```
Input prompt: ~5000 tokens (large task list + system prompt)
Max response: 8000 tokens
Required context window: â‰¥13000 tokens
```

### Recommended Settings

#### For Ollama (IMPORTANT!)

**Small models (7B-14B):**
```
Context Window: 8000 - 16000
Max Response Tokens: 4000 - 8000
```

**Medium models (20B-32B):**
```
Context Window: 32000 (default)
Max Response Tokens: 8000
```

**Large models (70B+):**
```
Context Window: 64000+
Max Response Tokens: 8000 - 12000
```

#### For Cloud Providers

**OpenAI:**
```
Context Window: 128000 (gpt-4o-mini)
Max Response Tokens: 8000
```

**Anthropic:**
```
Context Window: 200000 (Claude Sonnet)
Max Response Tokens: 8000
```

**OpenRouter:**
```
Context Window: Varies by model (see model specs)
Max Response Tokens: 8000
```

### Troubleshooting Context Length Errors

If you get errors like "context length exceeded":

**Option 1: Reduce Context Window (Ollama only)**
```
Try: 32000 â†’ 24000 â†’ 16000
```

**Option 2: Reduce Max Response Tokens**
```
Try: 8000 â†’ 6000 â†’ 4000
```

**Option 3: Reduce Input Size**
- Decrease max chat history
- Use fewer languages in semantic expansion
- Simplify system prompts

---

## ğŸ”§ Configuration Examples

### Configuration 1: Cost-Optimized
```yaml
Temperature: 0.1
Max Response Tokens: 4000
Context Window: 16000 (Ollama) / 128000 (Cloud)
```
**Best for:** Budget-conscious users, simple queries

### Configuration 2: Balanced (Default) â­
```yaml
Temperature: 0.1
Max Response Tokens: 8000
Context Window: 32000 (Ollama) / 128000+ (Cloud)
```
**Best for:** Most users, general usage

### Configuration 3: Maximum Quality
```yaml
Temperature: 0.1
Max Response Tokens: 12000
Context Window: 64000 (Ollama) / 200000 (Cloud)
```
**Best for:** Complex queries, detailed analysis

### Configuration 4: Ollama Small Model
```yaml
Temperature: 0.1
Max Response Tokens: 4000
Context Window: 8000
```
**Best for:** 7B-14B models with limited memory

---

## ğŸš¨ Common Issues

### Issue 1: Invalid JSON Output
**Symptoms:** Smart Search fails with parsing errors

**Solutions:**
1. âœ… Set temperature to 0.1
2. âœ… Ensure max response tokens â‰¥ 8000
3. âœ… Check model supports JSON output

### Issue 2: Truncated Responses
**Symptoms:** Task Chat responses cut off mid-sentence

**Solutions:**
1. âœ… Increase max response tokens (8000 â†’ 12000)
2. âœ… Reduce input size (fewer chat history messages)
3. âœ… Use shorter system prompts

### Issue 3: Context Length Errors (Ollama)
**Symptoms:** "context length exceeded" error

**Solutions:**
1. âœ… Reduce context window (32000 â†’ 16000)
2. âœ… Reduce max response tokens (8000 â†’ 6000)
3. âœ… Use a larger model

### Issue 4: Slow Responses
**Symptoms:** Queries take too long

**Solutions:**
1. âœ… Reduce max response tokens (8000 â†’ 4000)
2. âœ… Reduce context window
3. âœ… Use a faster/smaller model

---

## ğŸ“š Provider-Specific Documentation

### OpenAI
- Models: https://platform.openai.com/docs/models
- API Reference: https://platform.openai.com/docs/api-reference

### Anthropic
- Models: https://docs.anthropic.com/claude/docs/models-overview
- API Reference: https://docs.anthropic.com/claude/reference

### OpenRouter
- Models: https://openrouter.ai/models
- Documentation: https://openrouter.ai/docs

### Ollama
- Models: https://ollama.com/library
- API Reference: https://github.com/ollama/ollama/blob/main/docs/api.md
- Parameter Guide: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter

---

## ğŸ¯ Summary

### Key Takeaways

1. **Temperature 0.1** is essential for Smart Search and Task Chat reliability
2. **Max Response Tokens 8000** is the sweet spot for most users
3. **Context Window** matters primarily for Ollama users
4. **Always ensure:** Input + Response â‰¤ Context Window
5. **When in doubt:** Use default settings (they're optimized!)

### Quick Decision Tree

```
Are you using Ollama?
â”œâ”€ Yes â†’ Configure context window carefully
â”‚   â”œâ”€ Small model (7B-14B) â†’ 8000-16000
â”‚   â”œâ”€ Medium model (20B-32B) â†’ 32000
â”‚   â””â”€ Large model (70B+) â†’ 64000+
â””â”€ No â†’ Context window is informational only

Do you need detailed responses?
â”œâ”€ Yes â†’ Max tokens 12000-16000
â””â”€ No â†’ Max tokens 4000-8000 (faster & cheaper)

Experiencing issues?
â”œâ”€ JSON errors â†’ Temperature 0.1
â”œâ”€ Truncation â†’ Increase max tokens
â”œâ”€ Context errors â†’ Reduce window or tokens
â””â”€ Slow â†’ Reduce tokens or use smaller model
```

---

## ğŸ”— Related Documentation

- [Chat Modes Guide](CHAT_MODES.md)
- [Scoring System](SCORING_SYSTEM.md)
- [Settings Guide](SETTINGS_GUIDE.md)
- [Ollama Dev Documentation](dev/CRITICAL_OLLAMA_TOKEN_LIMIT_BUG_2025-01-24.md)
