# Ollama Setup Guide

Complete guide for setting up and using Ollama with Task Chat plugin.

---

## üìã What is Ollama?

Ollama is a free tool that allows you to run large language models (LLMs) locally on your computer. Unlike cloud providers (OpenAI, Anthropic), Ollama models run entirely offline with:

‚úÖ **Zero cost** - No API fees
‚úÖ **Complete privacy** - Data never leaves your computer  
‚úÖ **No internet required** - Works offline
‚úÖ **Full control** - Choose your model and parameters

**Trade-offs:**
- ‚ö†Ô∏è Slower than cloud providers (depends on your hardware)
- ‚ö†Ô∏è Requires good hardware
- ‚ö†Ô∏è Smaller models may have lower quality outputs

---

## üöÄ Installation

### Step 1: Install Ollama

**macOS / Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download installer from [ollama.com](https://ollama.com)

**Verify installation:**
```bash
ollama --version
```

### Step 2: Pull a Model

Download a model to use with Task Chat:

```bash
# Recommended models (balanced performance)
ollama pull qwen3:14b          # Good balance

# High-quality models (requires more RAM)
ollama pull qwen3:32b          # Better reasoning

# Specialized models
ollama pull deepseek-r1:32b      # Reasoning-focused
ollama pull gpt-oss:20b          # GPT-like responses
```

**Check installed models:**
```bash
ollama list
```

### Step 3: Configure CORS for Obsidian

Ollama needs to allow requests from Obsidian's app protocol.

**macOS (Ollama app):**
```bash
launchctl setenv OLLAMA_ORIGINS "app://obsidian.md*"
```
Then **restart Ollama** from the menu bar.

**macOS (Terminal):**
```bash
OLLAMA_ORIGINS="app://obsidian.md*" ollama serve
```

**Linux:**
Add to `/etc/systemd/system/ollama.service`:
```ini
[Service]
Environment="OLLAMA_ORIGINS=app://obsidian.md*"
```
Then:
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

**Windows (PowerShell):**
```powershell
$env:OLLAMA_ORIGINS="app://obsidian.md*"
ollama serve
```

### Step 4: Configure in Task Chat

1. Open Task Chat settings
2. Select **AI Provider**: Ollama
3. Set **API Endpoint**: `http://localhost:11434/api/chat` (default)
4. Select **Model**: Choose from your installed models
5. Click **Test connection** to verify

---

## üîß Troubleshooting

### Issue 1: Connection Failed

**Symptoms:**
```
Cannot connect to Ollama at http://localhost:11434/api/chat
```

**Solutions:**
1. ‚úÖ Check if Ollama is running:
   ```bash
   ollama list
   ```
2. ‚úÖ Start Ollama service:
   ```bash
   ollama serve
   ```
3. ‚úÖ Verify CORS configuration (see Step 3 above)
4. ‚úÖ Check endpoint in settings (default: `http://localhost:11434/api/chat`)

### Issue 2: Model Not Found

**Symptoms:**
```
Model 'qwen3:14b' not found in Ollama
```

**Solutions:**
1. ‚úÖ List installed models:
   ```bash
   ollama list
   ```
2. ‚úÖ Pull the model:
   ```bash
   ollama pull qwen3:14b
   ```
3. ‚úÖ Refresh available models in Task Chat settings
4. ‚úÖ Select a model from the dropdown

### Issue 3: Context Length Exceeded

**Symptoms:**
```
context length exceeded
```

**Solutions:**
1. ‚úÖ Reduce **Context Window** (32000 ‚Üí 16000)
2. ‚úÖ Reduce **Max Response Tokens** (8000 ‚Üí 6000)
3. ‚úÖ Reduce **Max Chat History** (in Task Chat settings)
4. ‚úÖ Use a larger model with bigger context window

### Issue 4: Slow Responses

**Symptoms:**
- Queries take 30+ seconds
- High CPU/memory usage

**Solutions:**
1. ‚úÖ Use a smaller model (32B ‚Üí 14B ‚Üí 8B)
2. ‚úÖ Reduce max response tokens
3. ‚úÖ Close other applications
4. ‚úÖ Upgrade hardware (more RAM/faster CPU)
5. ‚úÖ Consider using cloud providers for speed

### Issue 5: Poor Quality Results

**Symptoms:**
- Incorrect JSON format
- Missing relevant tasks
- Poor task analysis

**Solutions:**
1. ‚úÖ Use a larger model (8B ‚Üí 14B ‚Üí 32B)
2. ‚úÖ Ensure temperature is 0.1
3. ‚úÖ Increase context window
4. ‚úÖ Adjust filtering parameters (see Performance Tuning below)
5. ‚úÖ Consider cloud providers (OpenAI, Anthropic, OpenRouter)

### Issue 6: JSON Parsing Errors

**Symptoms:**
```
Query parsing error: Unexpected token
```

**Solutions:**
1. ‚úÖ Set temperature to 0.1 (lower = more consistent)
2. ‚úÖ Use newer models
3. ‚úÖ Increase max response tokens (model may be truncating)
4. ‚úÖ Try a different model
5. ‚úÖ Check model supports JSON output

---

## üîó Additional Resources

### Official Documentation

- **Ollama Website:** https://ollama.com
- **Model Library:** https://ollama.com/library
- **API Documentation:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **Modelfile Guide:** https://github.com/ollama/ollama/blob/main/docs/modelfile.md

### Task Chat Documentation

- **AI Provider Configuration:** [AI_PROVIDER_CONFIGURATION.md](AI_PROVIDER_CONFIGURATION.md)
- **Model Selection Guide:** [MODEL_SELECTION_GUIDE.md](MODEL_SELECTION_GUIDE.md)
- **Settings Guide:** [SETTINGS_GUIDE.md](SETTINGS_GUIDE.md)
